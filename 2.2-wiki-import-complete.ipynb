{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import pre-vectorized Wikipedia data\n",
    "\n",
    "Learn how to import large datasets with pre-computed embeddings. This notebook shows how to work with the Weaviate Wikipedia dataset that includes Snowflake Arctic embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate\n",
    "\n",
    "Connect to a Weaviate instance for importing pre-vectorized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weaviate\n",
    "import os\n",
    "# from weaviate.classes.init import AdditionalConfig, Timeout\n",
    "\n",
    "# client = weaviate.connect_to_custom(\n",
    "#     http_host=\"<http_host>\",\n",
    "#     http_port=\"<http_port>\",\n",
    "#     grpc_host=\"<grpc_host>\",\n",
    "#     grpc_port=\"<grpc_port>\",\n",
    "# )\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url=os.environ[\"WCD_TEST_URL\"],\n",
    "    auth_credentials=os.environ[\"WCD_TEST_KEY\"]\n",
    ")\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wikipedia collection\n",
    "\n",
    "Create a collection configured for pre-vectorized Wikipedia data with named vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Wiki collection with Snowflake Arctic embeddings\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "def create_wiki_collection():\n",
    "    # Delete existing collection if it exists\n",
    "    if client.collections.exists(\"Wiki\"):\n",
    "        client.collections.delete(\"Wiki\")\n",
    "\n",
    "    # Create collection for pre-vectorized Wikipedia data\n",
    "    client.collections.create(\n",
    "        name=\"Wiki\",\n",
    "\n",
    "        # Configure for pre-computed vectors with matching model\n",
    "        vector_config=[\n",
    "            Configure.Vectors.text2vec_weaviate(\n",
    "                name=\"main_vector\",\n",
    "                model=\"Snowflake/snowflake-arctic-embed-l-v2.0\",\n",
    "                source_properties=['title', 'text']  # Properties used for embedding\n",
    "            )\n",
    "        ],\n",
    "\n",
    "        # Define the schema for Wikipedia articles\n",
    "        properties=[\n",
    "            Property(name=\"wiki_id\", data_type=DataType.TEXT),\n",
    "            Property(name=\"title\", data_type=DataType.TEXT),\n",
    "            Property(name=\"text\", data_type=DataType.TEXT),\n",
    "            Property(name=\"url\", data_type=DataType.TEXT),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(\"Created Wiki collection with Snowflake Arctic embeddings\")\n",
    "\n",
    "create_wiki_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-vectorized Wikipedia dataset\n",
    "\n",
    "Load the Weaviate Wikipedia dataset with pre-computed Snowflake Arctic embeddings.\n",
    "\n",
    "[Dataset source](https://huggingface.co/datasets/weaviate/wiki-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading function prepared\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def prepare_dataset():\n",
    "    \"\"\"Load the pre-vectorized Wikipedia dataset\"\"\"\n",
    "    try:\n",
    "        # Try loading from local parquet files first\n",
    "        return load_dataset(\n",
    "            'parquet',\n",
    "            data_files={'train': ['wiki-data/weaviate/snowflake-arctic-v2/*.parquet']},\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "    except:\n",
    "        # Fallback to HuggingFace dataset\n",
    "        print(\"Loading from HuggingFace dataset (fallback)\")\n",
    "        return load_dataset(\n",
    "            \"weaviate/wiki-sample\",\n",
    "            \"weaviate-snowflake-arctic-v2\",\n",
    "            split=\"train\",\n",
    "            streaming=True\n",
    "        )\n",
    "\n",
    "print(\"Dataset loading function prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Wikipedia articles with pre-computed embeddings:\n",
      "============================================================\n",
      "\n",
      "Title: Unicode\n",
      "Wiki ID: 20231101.simple_64846_4\n",
      "Text preview: The Unicode Standard includes more than just the base code. Alongside the character encodings, the C...\n",
      "Vector dimensions: 1024\n",
      "URL: https://simple.wikipedia.org/wiki/Unicode\n",
      "\n",
      "Title: Book of Genesis\n",
      "Wiki ID: 20231101.simple_11278_4\n",
      "Text preview: The people of the world attempted to build a high tower (Tower of Babel) to show the power of mankin...\n",
      "Vector dimensions: 1024\n",
      "URL: https://simple.wikipedia.org/wiki/Book%20of%20Genesis\n",
      "\n",
      "Title: Rock Demers\n",
      "Wiki ID: 20231101.simple_864656_0\n",
      "Text preview: Rock Demers,  (December 11, 1933 – August 17, 2021) was a Canadian movie producer.  He was the found...\n",
      "Vector dimensions: 1024\n",
      "URL: https://simple.wikipedia.org/wiki/Rock%20Demers\n"
     ]
    }
   ],
   "source": [
    "# Preview the first few items to understand the data structure\n",
    "dataset = prepare_dataset()\n",
    "\n",
    "print(\"Sample Wikipedia articles with pre-computed embeddings:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "counter = 3\n",
    "for item in dataset:\n",
    "    print(f\"\\nTitle: {item['title']}\")\n",
    "    print(f\"Wiki ID: {item['wiki_id']}\")\n",
    "    print(f\"Text preview: {item['text'][:100]}...\")\n",
    "    print(f\"Vector dimensions: {len(item['vector'])}\")\n",
    "    print(f\"URL: {item['url']}\")\n",
    "\n",
    "    counter -= 1\n",
    "    if counter == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Wikipedia data with vectors\n",
    "\n",
    "Efficient batch import of pre-vectorized Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing up to 25,000 Wikipedia articles with embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing articles: 100%|█████████▉| 24999/25000 [00:47<00:00, 522.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Import completed successfully with no errors\n",
      "Successfully imported 25,000 Wikipedia articles\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "def import_wiki_data(max_rows=25000):\n",
    "    \"\"\"Import Wikipedia articles with their pre-computed vectors\"\"\"\n",
    "    print(f\"Importing up to {max_rows:,} Wikipedia articles with embeddings...\")\n",
    "\n",
    "    dataset = prepare_dataset()\n",
    "    wiki = client.collections.use(\"Wiki\")\n",
    "\n",
    "    counter = 0\n",
    "    error_threshold = 10\n",
    "\n",
    "    with wiki.batch.fixed_size(batch_size=500, concurrent_requests=2) as batch:\n",
    "        for item in tqdm(dataset, total=max_rows, desc=\"Importing articles\"):\n",
    "\n",
    "            # Prepare the article data\n",
    "            article_data = {\n",
    "                \"wiki_id\": item[\"wiki_id\"],\n",
    "                \"text\": item[\"text\"],\n",
    "                \"title\": item[\"title\"],\n",
    "                \"url\": item[\"url\"],\n",
    "            }\n",
    "\n",
    "            # Generate consistent UUID from wiki_id\n",
    "            article_uuid = generate_uuid5(item[\"wiki_id\"])\n",
    "\n",
    "            # Prepare the pre-computed vector\n",
    "            article_vector = {\n",
    "                \"main_vector\": item[\"vector\"]\n",
    "            }\n",
    "\n",
    "            # Add to batch\n",
    "            batch.add_object(\n",
    "                properties=article_data,\n",
    "                uuid=article_uuid,\n",
    "                vector=article_vector\n",
    "            )\n",
    "\n",
    "            # Check for errors during import\n",
    "            if batch.number_errors > error_threshold:\n",
    "                print(f\"\\nStopping import: reached {batch.number_errors} errors\")\n",
    "                break\n",
    "\n",
    "            # Stop when reaching max_rows limit\n",
    "            counter += 1\n",
    "            if counter >= max_rows:\n",
    "                break\n",
    "\n",
    "    # Final error check\n",
    "    failed_objects = wiki.batch.failed_objects\n",
    "    if len(failed_objects) > 0:\n",
    "        print(f\"\\nImport completed with {len(failed_objects)} errors\")\n",
    "        print(\"Sample error:\", failed_objects[-1])\n",
    "    else:\n",
    "        print(\"\\nImport completed successfully with no errors\")\n",
    "\n",
    "    print(f\"Successfully imported {counter:,} Wikipedia articles\")\n",
    "    return counter\n",
    "\n",
    "# Import the dataset\n",
    "imported_count = import_wiki_data(25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the import\n",
    "\n",
    "Check that articles were imported correctly with their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles in Wiki collection: 25,000\n",
      "Expected articles imported: 25,000\n",
      "Import success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Check total count in collection\n",
    "wiki = client.collections.use(\"Wiki\")\n",
    "total_articles = len(wiki)\n",
    "\n",
    "print(f\"Total articles in Wiki collection: {total_articles:,}\")\n",
    "print(f\"Expected articles imported: {imported_count:,}\")\n",
    "print(f\"Import success rate: {(total_articles/imported_count*100):.1f}%\" if imported_count > 0 else \"No articles to compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample imported articles:\n",
      "==================================================\n",
      "\n",
      "Article 1:\n",
      "  Title: Unicode\n",
      "  Wiki ID: 20231101.simple_64846_4\n",
      "  Text preview: The Unicode Standard includes more than just the base code. Alongside the character encodings, the C...\n",
      "  Vector dimensions: 1024\n",
      "  Vector sample: [-0.0174560546875, 0.041229248046875, -0.050750732421875, 0.03729248046875, 0.03704833984375]...\n",
      "\n",
      "Article 2:\n",
      "  Title: Book of Genesis\n",
      "  Wiki ID: 20231101.simple_11278_4\n",
      "  Text preview: The people of the world attempted to build a high tower (Tower of Babel) to show the power of mankin...\n",
      "  Vector dimensions: 1024\n",
      "  Vector sample: [0.04656982421875, 0.08062744140625, 0.031402587890625, -0.0224761962890625, -0.0400390625]...\n"
     ]
    }
   ],
   "source": [
    "# Verify article content and vectors\n",
    "response = wiki.query.fetch_objects(limit=2, include_vector=True)\n",
    "\n",
    "print(\"Sample imported articles:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, article in enumerate(response.objects, 1):\n",
    "    props = article.properties\n",
    "    vector = article.vector\n",
    "\n",
    "    print(f\"\\nArticle {i}:\")\n",
    "    print(f\"  Title: {props['title']}\")\n",
    "    print(f\"  Wiki ID: {props['wiki_id']}\")\n",
    "    print(f\"  Text preview: {props['text'][:100]}...\")\n",
    "    print(f\"  Vector dimensions: {len(vector['main_vector'])}\")\n",
    "    print(f\"  Vector sample: {vector['main_vector'][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick search test\n",
    "\n",
    "Verify the collection works with semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for 'space exploration NASA':\n",
      "=============================================\n",
      "\n",
      "1. Outer planet\n",
      "   Content: Trautman and Bethke – Human Outer Planet Exploration (2003) – NASA Langley Research Center and Princeton University...\n",
      "   URL: https://simple.wikipedia.org/wiki/Outer%20planet\n",
      "\n",
      "2. July 31\n",
      "   Content: 1999 – NASA intentionally crashes the Lunar Prospector spacecraft into the Moon, thus ending its mission to detect froze...\n",
      "   URL: https://simple.wikipedia.org/wiki/July%2031\n",
      "\n",
      "3. Space colonization\n",
      "   Content: Space colonization is the idea of humans living outside of Earth permanently. At present, there are no space colonies. H...\n",
      "   URL: https://simple.wikipedia.org/wiki/Space%20colonization\n"
     ]
    }
   ],
   "source": [
    "# Test semantic search on the imported data\n",
    "response = wiki.query.near_text(\n",
    "    query=\"space exploration NASA\",\n",
    "    limit=3,\n",
    "    target_vector=\"main_vector\"\n",
    ")\n",
    "\n",
    "print(\"Search results for 'space exploration NASA':\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for i, article in enumerate(response.objects, 1):\n",
    "    print(f\"\\n{i}. {article.properties['title']}\")\n",
    "    print(f\"   Content: {article.properties['text'][:120]}...\")\n",
    "    print(f\"   URL: {article.properties['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Creating collections for pre-vectorized data\n",
    "- Loading large datasets with embeddings from external sources\n",
    "- Efficient batch import with error handling\n",
    "- Verifying data integrity and search functionality\n",
    "\n",
    "The imported Wikipedia collection can now be used for various search and RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the client\n",
    "\n",
    "Always close your connection when finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202508-aws-genai-workshop-material",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
