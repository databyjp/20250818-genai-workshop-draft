{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a081d",
   "metadata": {},
   "source": [
    "## Working with PDFs with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ce690",
   "metadata": {},
   "source": [
    "PDFs contain more than rich formatting - they have images!\n",
    "\n",
    "<img src=\"data/imgs/hai_ai-index-report-2025_chapter2_excerpts_1_of_8.jpg\" width=\"250px\" />\n",
    "<img src=\"data/imgs/hai_ai-index-report-2025_chapter2_excerpts_4_of_8.jpg\" width=\"250px\" />\n",
    "<img src=\"data/imgs/hai_ai-index-report-2025_chapter2_excerpts_5_of_8.jpg\" width=\"250px\" />\n",
    "\n",
    "How do we work with these for RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d4de",
   "metadata": {},
   "source": [
    "### Approach 1 - Extract text and images separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0287c",
   "metadata": {},
   "source": [
    "Some libraries (like `docling`) can extract text and images from PDFs, and convert them into Markdown files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ea399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder = Path(\"data/pdfs\")\n",
    "output_dir = Path(\"data/parsed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23684011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: hai_ai-index-report-2025_chapter2_excerpts.pdf\n",
      "Converting document data/pdfs/hai_ai-index-report-2025_chapter2_excerpts.pdf to multimodal pages...\n",
      "Skipping data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs.md as it already exists.\n"
     ]
    }
   ],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling_core.types.doc import ImageRefMode\n",
    "\n",
    "IMAGE_RESOLUTION_SCALE = 2.0\n",
    "\n",
    "\n",
    "def parse_pdf_with_images(input_doc_path: Path, output_dir: Path):\n",
    "    # Reference: https://docling-project.github.io/docling/examples/export_figures/\n",
    "    md_filename = output_dir / f\"{input_doc_path.name.split('.')[0]}-parsed-w-imgs.md\"\n",
    "    if md_filename.exists():\n",
    "        print(f\"Skipping {md_filename} as it already exists.\")\n",
    "        return\n",
    "\n",
    "    pipeline_options = PdfPipelineOptions()\n",
    "    pipeline_options.images_scale = IMAGE_RESOLUTION_SCALE\n",
    "    pipeline_options.generate_picture_images = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    conv_res = doc_converter.convert(input_doc_path)\n",
    "\n",
    "    # Save markdown with embedded pictures\n",
    "    conv_res.document.save_as_markdown(md_filename, image_mode=ImageRefMode.REFERENCED)\n",
    "\n",
    "\n",
    "pdf_names = [f.name for f in data_folder.glob(\"hai*.pdf\") if f.is_file()]\n",
    "\n",
    "for pdf_fname in pdf_names:\n",
    "    print(f\"Processing file: {pdf_fname}\")\n",
    "\n",
    "    input_doc_path = data_folder / pdf_fname\n",
    "\n",
    "    print(f\"Converting document {input_doc_path} to multimodal pages...\")\n",
    "    parse_pdf_with_images(input_doc_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e8a60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chapter 2: Technical Performance\n",
      "\n",
      "## RAG: Retrieval Augment Generation (RAG)\n",
      "\n",
      "An  increasingly  common  capability  being  tested  in  LLMs is retrieval-augmented generation (RAG). This approach integrates LLMs  with retrieval mechanisms  to enhance their  response  generation. The  model fi rst  retrieves  relevant information  from fi les  or  documents  and  then  generates  a response tailored to the user's query based on the retrieved content.  RAG  has  diverse  use  cases,  including  answering precise questions from large databases and addressing customer queries using information from company documents.\n",
      "\n",
      "models. 2024 also saw the release of numerous benchmarks for  evaluating  RAG  systems,  including  Ragnarok  (a  RAG arena battleground) and CRAG (Comprehensive RAG benchmark). Additionally, specialized RAG benchmarks, such as FinanceBench for fi nancial question answering, have been developed to address speci fi c use cases.\n",
      "\n",
      "## Berkeley Function Calling Leaderboard\n",
      "\n",
      "In r\n"
     ]
    }
   ],
   "source": [
    "md_filepath = Path(\"data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs.md\")\n",
    "md_txt = md_filepath.read_text()\n",
    "print(md_txt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0297db",
   "metadata": {},
   "source": [
    "#### Chunking text files with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee9281",
   "metadata": {},
   "source": [
    "More complex than just text, since we need to handle images as well.\n",
    "\n",
    "- Must include entire image string in the chunk\n",
    "- When vectorizing, optionally include base64 of image\n",
    "    - Your embedding model must be multimodal\n",
    "\n",
    "Chunking becomes more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48bb25",
   "metadata": {},
   "source": [
    "One method: try a specialized library like `chonkie` to handle this\n",
    "\n",
    "Chonkie offers a variety of chunking strategies:\n",
    "\n",
    "<img src=\"images/chonkie_methods.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeea62c",
   "metadata": {},
   "source": [
    "There isn't going to be a \"one size fits all\" solution for chunking PDFs with images. But these libraries can help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daef5ae",
   "metadata": {},
   "source": [
    "Let's try a couple of different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7adb5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import RecursiveChunker\n",
    "\n",
    "# Initialize the recursive chunker to chunk Markdown\n",
    "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d61fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts = chunker.chunk(md_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8d48b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Token count: 1681\n",
      "Chunk text:\n",
      "    ## Chapter 2: Technical Performance  ## RAG: Retrieval Augment Generation (RAG)\n",
      "    An  increasingly  common  capability  being  tested  in  LLMs is retrieval-\n",
      "    augmented generation (RAG). This approach integrates LLMs  with retrieval\n",
      "    mechanisms  to enhance their  response  generation. The  model fi rst  retrieves\n",
      "    relevant information  from fi les  or  documents  and  then  generates  a\n",
      "    response tailored to the user's query based on the retrieved content.  RAG  has\n",
      "    diverse  use  cases,  including  ...\n",
      "\n",
      "========================================\n",
      "Token count: 1793\n",
      "Chunk text:\n",
      "    ## Berkeley Function-Calling Leaderboard Evaluation Data Composition\n",
      "    ![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_ar\n",
      "    tifacts/image_000000_7d6b2eec7e2a34253b28c23b7501ca8fbb4a6b25b9f0adb845f99f865bb\n",
      "    f518a.png)  Figure 2.2.17 9  | Javascript (AST)          | Chatting Capability\n",
      "    | |---------------------------|---------------------------------| | 2.5%\n",
      "    | 10.0%                           | | SQL (AST) 5.0%            | Simple (Exec)\n",
      "    ...\n",
      "\n",
      "========================================\n",
      "Token count: 1563\n",
      "Chunk text:\n",
      "    ## Chapter 2: Technical Performance  2.2 Language  The top model on the Berkeley\n",
      "    Function Calling Leaderboard is  watt-tool-70b,  a fi ne-tuned  variant  of\n",
      "    Llama-3.3-70BInstruct designed speci fi cally for function calling. It achieved\n",
      "    an overall accuracy of 74.31 (Figure 2.2.18). The next-highestscoring  model\n",
      "    was  a  November variant  of  GPT-4o,  with  a score of 72.08. Performance on\n",
      "    this benchmark has improved signi fi cantly over the course of 2024, with top\n",
      "    models at the end of the yea...\n",
      "\n",
      "========================================\n",
      "Token count: 1579\n",
      "Chunk text:\n",
      "    ## Tasks in the MTEB benchmark  Source: Muennigho ff et al., 2023\n",
      "    ![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_ar\n",
      "    tifacts/image_000004_9582114380c8aaf00b26451127c66e9ed0dda334856ff64c9cb38b473fb\n",
      "    586c5.png)  Figure 2.2.19  ![Image](data/parsed/hai_ai-index-\n",
      "    report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000005_33bf09ad90ca2\n",
      "    e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png)  transforms it into an\n",
      "    embedding vector. This transformation enables the  m...\n",
      "\n",
      "========================================\n",
      "Token count: 1651\n",
      "Chunk text:\n",
      "    ## 2.8 AI Agents  For decades, the topic of AI agents has been widely discussed\n",
      "    in the AI community, yet few benchmarks have achieved widespread adoption,\n",
      "    including those featured in last year's Index, such as AgentBench and\n",
      "    MLAgentBench. This is partly due to the  inherent  complexity  of  benchmarking\n",
      "    agentic tasks, which  are typically  more diverse,  dynamic,  and  variable\n",
      "    than  tasks  like  image  classi fi cation  or  answering language questions. As\n",
      "    AI continues to evolve, it will beco...\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991dde5e",
   "metadata": {},
   "source": [
    "Let's try a \"semantic\" chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc60d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=2048,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29aed757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text into `chunk_texts` as we've done before\n",
    "# BEGIN_SOLUTION\n",
    "chunk_texts = chunker.chunk(md_txt)\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7042aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Token count: 441\n",
      "Chunk text:\n",
      "    ## Chapter 2: Technical Performance  ## RAG: Retrieval Augment Generation (RAG)\n",
      "    An  increasingly  common  capability  being  tested  in  LLMs is retrieval-\n",
      "    augmented generation (RAG). This approach integrates LLMs  with retrieval\n",
      "    mechanisms  to enhance their  response  generation. The  model fi rst  retrieves\n",
      "    relevant information  from fi les  or  documents  and  then  generates  a\n",
      "    response tailored to the user's query based on the retrieved content.  RAG  has\n",
      "    diverse  use  cases,  including  ...\n",
      "\n",
      "========================================\n",
      "Token count: 778\n",
      "Chunk text:\n",
      "    |---------------------------|---------------------------------| | 2.5%\n",
      "    | 10.0%                           | | SQL (AST) 5.0%            | Simple (Exec)\n",
      "    | | Java (AST)                | 5.0% Multiple (Exec)            | | 5.0% REST\n",
      "    (Exec)          | 2.5% Parallel (Exec)            | | Relevance 12.0%\n",
      "    | 2.5% Parallel & Multiple (Exec) | | Parallel & Multiple (AST) | Simple (AST)\n",
      "    20.0%              | |                           | 2.0%                  ...\n",
      "\n",
      "========================================\n",
      "Token count: 131\n",
      "Chunk text:\n",
      "     ## MTEB: Massive Text Embedding Benchmark  The  Massive Text  Embedding\n",
      "    Benchmark  (MTEB),  created by a team at Hugging Face and Cohere, was introduced\n",
      "    in late 2022 to comprehensively evaluate how models perform on various embedding\n",
      "    tasks. Embedding involves converting data,  such  as  words,  texts,  or\n",
      "    documents,  into  numerical vectors that capture rough semantic meanings and\n",
      "    distance between vectors. Embedding is an essential component of RAG. During a\n",
      "    RAG task, when users input a query...\n",
      "\n",
      "========================================\n",
      "Token count: 377\n",
      "Chunk text:\n",
      "     ![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_a\n",
      "    rtifacts/image_000004_9582114380c8aaf00b26451127c66e9ed0dda334856ff64c9cb38b473f\n",
      "    b586c5.png)  Figure 2.2.19  ![Image](data/parsed/hai_ai-index-\n",
      "    report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000005_33bf09ad90ca2\n",
      "    e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png)  transforms it into an\n",
      "    embedding vector. This transformation enables the  model to then  search for\n",
      "    relevant  information. MTEB  includes  5...\n",
      "\n",
      "========================================\n",
      "Token count: 147\n",
      "Chunk text:\n",
      "     ![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_a\n",
      "    rtifacts/image_000006_60957158f1c4e3002ec6ab1ce22a95dd9fb64fde5d2273cea56877a94e\n",
      "    3a7957.png)  ## 2.8 AI Agents  For decades, the topic of AI agents has been\n",
      "    widely discussed in the AI community, yet few benchmarks have achieved\n",
      "    widespread adoption, including those featured in last year's Index, such as\n",
      "    AgentBench and MLAgentBench. ...\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2a8a4",
   "metadata": {},
   "source": [
    "### Set up Weaviate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22933c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "client = weaviate.connect_to_embedded(\n",
    "    version=\"1.32.0\",\n",
    "    headers={\n",
    "        \"X-Cohere-Api-Key\": os.getenv(\"COHERE_API_KEY\"),\n",
    "    },\n",
    "    environment_variables={\"LOG_LEVEL\": \"error\"}  # Reduce amount of logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842550b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"Chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e202bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<weaviate.collections.collection.sync.Collection at 0x157b8fd30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Chunks\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"document_title\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk_number\",\n",
    "            data_type=DataType.INT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"filename\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.FIELD\n",
    "        ),\n",
    "    ],\n",
    "    vector_config=[\n",
    "        Configure.Vectors.text2vec_cohere(\n",
    "            name=\"default\",\n",
    "            source_properties=[\"document_title\", \"chunk\"],\n",
    "            model=\"embed-v4.0\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c9e2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = client.collections.use(\"Chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f3bad",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dee53991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:00, 8363.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with chunks.batch.fixed_size(batch_size=100) as batch:\n",
    "    for i, chunk_text in tqdm(enumerate(chunk_texts)):\n",
    "        obj = {\n",
    "            \"document_title\": \"HAI AI Index Report 2025\",\n",
    "            \"filename\": \"data/pdfs/hai_ai-index-report-2025_chapter2_excerpts.pdf\",\n",
    "            \"chunk\": chunk_text.text,\n",
    "            \"chunk_number\": i + 1,\n",
    "        }\n",
    "\n",
    "        # Add object to batch for import with (batch.add_object())\n",
    "        # BEGIN_SOLUTION\n",
    "        batch.add_object(\n",
    "            properties=obj\n",
    "        )\n",
    "        # END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed8bc9",
   "metadata": {},
   "source": [
    "### RAG queries\n",
    "\n",
    "How do we perform RAG in this scenario? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e0fed",
   "metadata": {},
   "source": [
    "This is a bit different, because we haven't embedded the images (or stored them in Weaviate).\n",
    "\n",
    "In this scenario, let's:\n",
    "\n",
    "- Retrieve text chunks\n",
    "- Get images referred to in the text\n",
    "- Convert the images to base64\n",
    "- Send (retrieved text + images + prompt) to LLM for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cae8dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "\n",
      "China's  self-driving  revolution  is  also  accelerating,  led  by companies like Baidu's Apollo Go, which reported 988,000 rides across China in Q3 2024, re fl ecting a 20% year-over-year increase. In October 2024, the company was operating 400 robotaxis and announced plans to expand its fl eet to 1,000 by the end of 2025. Pony.AI, another Chinese autonomous vehicle manufacturer, has pledged to scale its robotaxi fl eet from 200 to at least 1,000 vehicles-with expectations that the fl eet will reach 2,000 to 3,000 by the end of 2026. China is leading the way in autonomous vehicle testing, with reports indicating  that  it  is  testing  more  driverless  cars  than  any other country and currently rolling them out across 16 cities. Robotaxis  in  China  are  notably  a ff ordable-even  cheaper, in  some  cases,  than  rides  provided  by  human  drivers.  To support  this growth, China  has prioritized establishing national regulations to govern the deployment of driverless cars.  Be...\n",
      "\n",
      "========================================\n",
      "\n",
      "Self-driving vehicles have long been a goal for AI researchers and technologists. However, their widespread adoption has been  slower  than  anticipated.  Despite  many  predictions that  fully  autonomous  driving  is  imminent,  widespread  use of  self-driving  vehicles  has yet  to  become  a  reality.  Still,  in recent  years,  signi fi cant  progress  has  been  made.  In  cities like  San  Francisco  and  Phoenix, fl eets  of  self-driving  taxis are  now  operating  commercially.  This  section  examines recent advancements in autonomous driving, focusing on deployment, technological breakthroughs and new benchmarks, safety performance, and policy challenges.\n",
      "\n",
      "## Deployment\n",
      "\n",
      "Self-driving cars are increasingly being deployed worldwide. Cruise, a subsidiary of General Motors, launched its autonomous vehicles  in  San  Francisco  in  late  2022  before having its license suspended in 2023 after a litany of safety incidents. Waymo, a subsidiary of Alphabet, began deploying its  r...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000011_60957158f1c4e3002ec6ab1ce22a95dd9fb64fde5d2273cea56877a94e3a7957.png)\n",
      "\n",
      "32-hour budget, humans outperform AI by a factor of two. The  researchers  also  note  that  for  certain  tasks,  AI  agents already  demonstrate  expertise  comparable  to  humans  but can  deliver  results  signi fi cantly  faster  and  at  a  lower  cost. For example, AI agents can write custom Triton kernels more quickly than any human expert.\n",
      "\n",
      "RE-Bench: average normalized score@k\n",
      "\n",
      "Source: Wijk et al., 2024 | Chart: 2025 AI Index report\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000012_90c7c87f35758c0ce8c4a95f97089830098242f6ae4df85ab58cd0aa4f31fa8b.png)\n",
      "\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "## Self-Driving Cars\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000014_33bf09ad90ca2e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png)\n",
      "\n",
      "Source: Verge, 2024\n",
      "\n",
      "Figure 2.9.12\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000015_f28da9504e59323964f966e5d2887aff0bca0dc2998b4688976fc0055b21483a.png)\n",
      "\n",
      "Most existing benchmarks  for end-to-end autonomous driving rely on open-loop evaluation, which can be restrictive. Open-loop settings fail to test how autonomous agents  react  to  real-world  conditions  and  often  lead  to models that memorize driving patterns rather than learning to  drive  authentically.  While  closed-loop  benchmarks  like Town05Long and Longest6 exist, they primarily assess basic driving skills rather than performance in complex, interactive scenarios.  Bench2Drive  is  another  new  benchmark  that improves on these limitations by providing a comprehensive, realistic, closed-loop testi...\n",
      "\n",
      "========================================\n",
      "\n",
      "13 This metric accounts for both route completion and infractions, averaging route completion percentages while applying penalties based on infraction severity. For more detail on the driving score methodology, see Section 3 of the Bench2Drive paper ....\n",
      "\n",
      "========================================\n",
      "\n",
      "Figure 2.9.13\n",
      "\n",
      "Baidu's RT-6\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000007_3921e74bcd236b403bab6328e8a11a403024116e502a0d0bbf5264f43935b867.png)\n",
      "\n",
      "## 2.8 AI Agents Chapter 2: Technical Performance\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000008_33bf09ad90ca2e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png)\n",
      "\n",
      "VAB presents a signi fi cant challenge for AI systems. The topperforming model, GPT-4o, achieves an overall success rate of just 36.2%, while most proprietary language models average around  20%  (Figure  2.8.2).  According  to  the  benchmark's authors, these results reveal that current AI models are far from ready for direct deployment in agentic settings.\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000009_8670c86bc1f4a1d41097f5ec286999521dc5bbf65615ebf4fb76a6a03b679acd.png)\n",
      "\n",
      "Figure 2.8.2\n",
      "\n",
      "## RE-Bench\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000016_c8bc31d8f68c8c7ddb890160760f994e128d339f00f23606bd9cf2fc81bd1a57.png)\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000017_bee67d2908e5fb2a02cb00e353ce5175b52b2b54e1b2aad6859821f20efb317c.png)\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000018_f81c011205d986c7c7b9373917b9f59beb1a5844aae86348b1f3510389321526.png)\n",
      "\n",
      "2. Quasi-Realistic Scenario Closed-Loop (E2E) Evaluation\n",
      "\n",
      "Multi-dimensional Ability Assessment\n",
      "...\n",
      "\n",
      "========================================\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000004_9582114380c8aaf00b26451127c66e9ed0dda334856ff64c9cb38b473fb586c5.png)\n",
      "\n",
      "Figure 2.2.19\n",
      "\n",
      "![Image](data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs_artifacts/image_000005_33bf09ad90ca2e078ea69ec3e2e6beeea2fb2395ebd182a5b6e68c8d215648de.png)\n",
      "\n",
      "transforms it into an embedding vector. This transformation enables the  model to then  search for  relevant  information. MTEB  includes  58  datasets  spanning  112  languages  and eight embedding tasks (Figure 2.2.19). 10  For example, in the bitext mining task, there are two sets of sentences from two di ff erent languages, and for every sentence in the fi rst  set, the model is tasked to fi nd the best match in the second set.\n",
      "\n",
      "## 2.8 AI Agents Chapter 2: Technical Performance\n",
      "\n",
      "AI agents, autonomous or semiautonomous systems designed to operate within  speci fi c  environments to accomplish goals, represent an  exciting  fronti...\n",
      "\n",
      "========================================\n",
      "## Chapter 2: Technical Performance\n",
      "\n",
      "## RAG: Retrieval Augment Generation (RAG)\n",
      "\n",
      "An  increasingly  common  capability  being  tested  in  LLMs is retrieval-augmented generation (RAG). This approach integrates LLMs  with retrieval mechanisms  to enhance their  response  generation. The  model fi rst  retrieves  relevant information  from fi les  or  documents  and  then  generates  a response tailored to the user's query based on the retrieved content.  RAG  has  diverse  use  cases,  including  answering precise questions from large databases and addressing customer queries using information from company documents.\n",
      "\n",
      "models. 2024 also saw the release of numerous benchmarks for  evaluating  RAG  systems,  including  Ragnarok  (a  RAG arena battleground) and CRAG (Comprehensive RAG benchmark). Additionally, specialized RAG benchmarks, such as FinanceBench for fi nancial question answering, have been developed to address speci fi c use cases.\n",
      "\n",
      "## Berkeley Function Calling Leaderboard\n",
      "\n",
      "In r...\n"
     ]
    }
   ],
   "source": [
    "response = chunks.query.hybrid(\n",
    "    query=\"Latest in self-driving cars\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(o.properties[\"chunk\"][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a729583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_paths(text):\n",
    "    \"\"\"Extract image paths from markdown-style image references.\"\"\"\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d036efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_base64s(image_paths, base_path=None):\n",
    "    import base64\n",
    "    base64_images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = Path(base_path) / img_path if base_path else Path(img_path)\n",
    "        image_bytes = full_path.read_bytes()\n",
    "        base64_string = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        base64_images.append(base64_string)\n",
    "\n",
    "    return base64_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "430397bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = \"\"\n",
    "all_images = []\n",
    "\n",
    "for o in response.objects:\n",
    "    chunk_text = o.properties[\"chunk\"]\n",
    "    image_paths = extract_image_paths(chunk_text)\n",
    "    all_images.extend(get_image_base64s(image_paths, base_path=\"data/parsed\"))\n",
    "\n",
    "    all_chunks += \"\\n\\n\" + chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d93da885",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_text = \"\"\"\n",
    "What does this tell us about the latest in self-driving cars\n",
    "\n",
    "Describe the details from the figures as well, if necessary.\n",
    "\"\"\" + \"\\n\\n\" + all_chunks\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": task_text}\n",
    "    ]\n",
    "}\n",
    "\n",
    "for img in all_images:\n",
    "    content = {\n",
    "        \"type\": \"image\",\n",
    "        \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": \"image/png\",\n",
    "            \"data\": img,\n",
    "        }\n",
    "    }\n",
    "    # Append `content`` to message[\"content\"]\n",
    "    # BEGIN_SOLUTION\n",
    "    message[\"content\"].append(content)\n",
    "    # END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ee4c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "anthropic_response = anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    # Add [message] as the messages to pass to Claude\n",
    "    # BEGIN_SOLUTION\n",
    "    messages=[message]\n",
    "    # END_SOLUTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "754f2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on this comprehensive report on self-driving cars, here are the key developments in autonomous vehicle technology:\n",
      "\n",
      "## Global Deployment Progress\n",
      "\n",
      "**United States:**\n",
      "- **Waymo** leads commercially, operating in 4 major cities (Phoenix, San Francisco, Los Angeles, Austin)\n",
      "- Provides 150,000 paid rides per week, covering over 1 million miles monthly\n",
      "- Plans to expand testing to 10 additional cities, including challenging weather conditions (snowy regions)\n",
      "- **Cruise** faced setbacks with license suspension in 2023 due to safety incidents\n",
      "\n",
      "**China:**\n",
      "- Experiencing rapid growth with companies like **Baidu's Apollo Go** reporting 988,000 rides in Q3 2024 (20% year-over-year increase)\n",
      "- **Pony.AI** scaling from 200 to 1,000+ vehicles, targeting 2,000-3,000 by 2026\n",
      "- Testing across 16 cities - more than any other country\n",
      "- Robotaxis are notably affordable, sometimes cheaper than human drivers\n",
      "\n",
      "## Major Technical Innovations\n",
      "\n",
      "**New Vehicles:**\n",
      "- **Tesla Cybercab**: Two-passenger autonomous vehicle without steering wheel/pedals, planned for 2026 production under $30,000\n",
      "- **Tesla Robovan**: Electric autonomous van for up to 20 passengers\n",
      "- **Baidu RT6**: Latest-generation robotaxi priced at just $30,000 with battery-swapping system\n",
      "\n",
      "## Advanced Benchmarking Systems\n",
      "\n",
      "The report details several new evaluation frameworks:\n",
      "\n",
      "**Bench2Drive** (Figure 2.9.13 shows its comprehensive structure):\n",
      "- Provides realistic, closed-loop testing environment\n",
      "- Includes 2+ million annotated training frames from 10,000+ clips\n",
      "- Features 220 diverse test routes\n",
      "- Addresses limitations of previous open-loop benchmarks that failed to test real-world adaptability\n",
      "\n",
      "**Other New Benchmarks:**\n",
      "- **nuPlan**: Large-scale dataset with 1,282 hours of diverse driving scenarios\n",
      "- **OpenAD**: First real-world benchmark for 3D object detection with focus on domain generalization\n",
      "\n",
      "## Performance Data\n",
      "\n",
      "From the driving performance chart (Figure 2.9.14), current autonomous driving methods show varying capabilities, with the benchmark revealing significant room for improvement across different systems.\n",
      "\n",
      "## Key Challenges and Outlook\n",
      "\n",
      "- **Slower adoption** than originally predicted, despite technological advances\n",
      "- **Safety concerns** remain paramount (as evidenced by Cruise's suspension)\n",
      "- **Freight applications** showing progress with companies like Aurora and Kodiak, though commercial launches face delays\n",
      "- **Cost reduction** is accelerating adoption potential\n",
      "- **Regulatory frameworks** being established, particularly in China\n",
      "\n",
      "The report suggests that while fully autonomous driving isn't yet widespread, significant commercial progress is being made in specific markets, with China and the US leading in different approaches to deployment and regulation.\n"
     ]
    }
   ],
   "source": [
    "print(anthropic_response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54d3533e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"build_git_commit\":\"7cebee0421\",\"build_go_version\":\"go1.24.5\",\"build_image_tag\":\"HEAD\",\"build_wv_version\":\"1.32.0\",\"error\":\"context canceled\",\"level\":\"error\",\"msg\":\"replication engine failed to start after FSM caught up\",\"time\":\"2025-08-26T17:48:43+01:00\"}\n",
      "{\"build_git_commit\":\"7cebee0421\",\"build_go_version\":\"go1.24.5\",\"build_image_tag\":\"HEAD\",\"build_wv_version\":\"1.32.0\",\"error\":\"cannot find peer\",\"level\":\"error\",\"msg\":\"transferring leadership\",\"time\":\"2025-08-26T17:48:43+01:00\"}\n"
     ]
    }
   ],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67fd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "202508-aws-genai-workshop-material",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
