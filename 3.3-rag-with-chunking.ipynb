{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4a081d",
   "metadata": {},
   "source": [
    "## Working with PDFs with images\n",
    "\n",
    "PDFs contain more than rich formatting - they have images!\n",
    "\n",
    "Run the cells below to convert PDF data to images (this should take about a minute). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a0028a-e09e-4836-a3b1-9f0bdff89bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T13:50:09.950414Z",
     "iopub.status.busy": "2025-09-04T13:50:09.950160Z",
     "iopub.status.idle": "2025-09-04T13:50:10.057749Z",
     "shell.execute_reply": "2025-09-04T13:50:10.056199Z",
     "shell.execute_reply.started": "2025-09-04T13:50:09.950393Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pymupdf\n",
    "except ImportError:\n",
    "    %pip install -Uqq pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d263ecf2-ee73-443b-9a7f-51e446b4c87d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T13:50:11.869629Z",
     "iopub.status.busy": "2025-09-04T13:50:11.869353Z",
     "iopub.status.idle": "2025-09-04T13:50:56.675050Z",
     "shell.execute_reply": "2025-09-04T13:50:56.674394Z",
     "shell.execute_reply.started": "2025-09-04T13:50:11.869606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data/pdfs/amazon-2025-08-8k-excerpts.pdf to images...\n",
      "Converted 12 pages to images\n",
      "Converting data/pdfs/hai_ai-index-report-2025_chapter2_excerpts.pdf to images...\n",
      "Converted 8 pages to images\n",
      "Converting data/pdfs/reddit_s1a.pdf to images...\n",
      "Converted 288 pages to images\n",
      "Images extracted from PDFs\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python pdf_to_img.py\n",
    "echo \"Images extracted from PDFs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c46a0b-56dd-44bd-a331-dc693b0625d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "images = [\n",
    "    f\"data/imgs/hai_ai-index-report-2025_chapter2_excerpts_1_of_8.jpg\",\n",
    "    f\"data/imgs/hai_ai-index-report-2025_chapter2_excerpts_4_of_8.jpg\",\n",
    "    f\"data/imgs/hai_ai-index-report-2025_chapter2_excerpts_5_of_8.jpg\"   \n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 40))\n",
    "\n",
    "for i, img_path in enumerate(images):\n",
    "    img = Image.open(img_path)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ce690",
   "metadata": {},
   "source": [
    "How do we work with these for RAG?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d4de",
   "metadata": {},
   "source": [
    "### Approach 1 - Extract text and images separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0287c",
   "metadata": {},
   "source": [
    "Some libraries (like `docling`) can extract text and images from PDFs, and convert them into Markdown files.\n",
    "\n",
    "Here, we've pre-converted this PDF into markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_filepath = Path(\"data/parsed/hai_ai-index-report-2025_chapter2_excerpts-parsed-w-imgs.md\")\n",
    "md_txt = md_filepath.read_text()\n",
    "print(md_txt[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0297db",
   "metadata": {},
   "source": [
    "#### Chunking text files with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee9281",
   "metadata": {},
   "source": [
    "More complex than just text, since we need to handle images as well.\n",
    "\n",
    "- Must include entire image string in the chunk\n",
    "- When vectorizing, optionally include base64 of image\n",
    "    - Your embedding model must be multimodal\n",
    "\n",
    "Chunking becomes more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48bb25",
   "metadata": {},
   "source": [
    "One method: try a specialized library like `chonkie` to handle this\n",
    "\n",
    "Chonkie offers a variety of chunking strategies:\n",
    "\n",
    "<img src=\"images/chonkie_methods.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeea62c",
   "metadata": {},
   "source": [
    "There isn't going to be a \"one size fits all\" solution for chunking PDFs with images. But these libraries can help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daef5ae",
   "metadata": {},
   "source": [
    "Let's try a couple of different approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import RecursiveChunker\n",
    "\n",
    "# Initialize the recursive chunker to chunk Markdown\n",
    "chunker = RecursiveChunker.from_recipe(\"markdown\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_texts = chunker.chunk(md_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991dde5e",
   "metadata": {},
   "source": [
    "Let's try a \"semantic\" chunker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=2048,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aed757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text into `chunk_texts` as we've done before\n",
    "# BEGIN_SOLUTION\n",
    "chunk_texts = chunker.chunk(md_txt)\n",
    "# END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7042aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunk_texts[:5]:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Chunk text:\")\n",
    "    wrapped_text = textwrap.fill(chunk.text[:500]+\"...\", width=80)\n",
    "    print(textwrap.indent(wrapped_text, \"    \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a2a8a4",
   "metadata": {},
   "source": [
    "### Set up Weaviate Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22933c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "client = weaviate.connect_to_embedded(\n",
    "    version=\"1.32.0\",\n",
    "    headers={\n",
    "        \"X-Cohere-Api-Key\": os.getenv(\"COHERE_API_KEY\"),\n",
    "    },\n",
    "    environment_variables={\"LOG_LEVEL\": \"error\"}  # Reduce amount of logs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842550b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.collections.delete(\"Chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e202bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
    "\n",
    "client.collections.create(\n",
    "    name=\"Chunks\",\n",
    "    properties=[\n",
    "        Property(\n",
    "            name=\"document_title\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk\",\n",
    "            data_type=DataType.TEXT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"chunk_number\",\n",
    "            data_type=DataType.INT,\n",
    "        ),\n",
    "        Property(\n",
    "            name=\"filename\",\n",
    "            data_type=DataType.TEXT,\n",
    "            tokenization=Tokenization.FIELD\n",
    "        ),\n",
    "    ],\n",
    "    vector_config=[\n",
    "        Configure.Vectors.text2vec_cohere(\n",
    "            name=\"default\",\n",
    "            source_properties=[\"document_title\", \"chunk\"],\n",
    "            model=\"embed-v4.0\"\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = client.collections.use(\"Chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f3bad",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee53991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "with chunks.batch.fixed_size(batch_size=100) as batch:\n",
    "    for i, chunk_text in tqdm(enumerate(chunk_texts)):\n",
    "        obj = {\n",
    "            \"document_title\": \"HAI AI Index Report 2025\",\n",
    "            \"filename\": \"data/pdfs/hai_ai-index-report-2025_chapter2_excerpts.pdf\",\n",
    "            \"chunk\": chunk_text.text,\n",
    "            \"chunk_number\": i + 1,\n",
    "        }\n",
    "\n",
    "        # Add object to batch for import with (batch.add_object())\n",
    "        # BEGIN_SOLUTION\n",
    "        batch.add_object(\n",
    "            properties=obj\n",
    "        )\n",
    "        # END_SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed8bc9",
   "metadata": {},
   "source": [
    "### RAG queries\n",
    "\n",
    "How do we perform RAG in this scenario? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e0fed",
   "metadata": {},
   "source": [
    "This is a bit different, because we haven't embedded the images (or stored them in Weaviate).\n",
    "\n",
    "In this scenario, let's:\n",
    "\n",
    "- Retrieve text chunks\n",
    "- Get images referred to in the text\n",
    "- Convert the images to base64\n",
    "- Send (retrieved text + images + prompt) to LLM for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chunks.query.hybrid(\n",
    "    query=\"Latest in self-driving cars\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(f\"\\n\" + \"=\" * 40)\n",
    "    print(o.properties[\"chunk\"][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a729583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_image_paths(text):\n",
    "    \"\"\"Extract image paths from markdown-style image references.\"\"\"\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d036efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_base64s(image_paths, base_path=None):\n",
    "    import base64\n",
    "    base64_images = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = Path(base_path) / img_path if base_path else Path(img_path)\n",
    "        image_bytes = full_path.read_bytes()\n",
    "        base64_string = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "        base64_images.append(base64_string)\n",
    "\n",
    "    return base64_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430397bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = \"\"\n",
    "all_images = []\n",
    "\n",
    "for o in response.objects:\n",
    "    chunk_text = o.properties[\"chunk\"]\n",
    "    image_paths = extract_image_paths(chunk_text)\n",
    "    all_images.extend(get_image_base64s(image_paths, base_path=\"data/parsed\"))\n",
    "\n",
    "    all_chunks += \"\\n\\n\" + chunk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93da885",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_text = \"\"\"\n",
    "What does this tell us about the latest in self-driving cars\n",
    "\n",
    "Describe the details from the figures as well, if necessary.\n",
    "\"\"\" + \"\\n\\n\" + all_chunks\n",
    "\n",
    "message = {\n",
    "    \"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"text\", \"text\": task_text}\n",
    "    ]\n",
    "}\n",
    "\n",
    "for img in all_images:\n",
    "    content = {\n",
    "        \"type\": \"image\",\n",
    "        \"source\": {\n",
    "            \"type\": \"base64\",\n",
    "            \"media_type\": \"image/png\",\n",
    "            \"data\": img,\n",
    "        }\n",
    "    }\n",
    "    # Append `content`` to message[\"content\"]\n",
    "    # BEGIN_SOLUTION\n",
    "    message[\"content\"].append(content)\n",
    "    # END_SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "anthropic_response = anthropic.Anthropic().messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    # Add [message] as the messages to pass to Claude\n",
    "    # BEGIN_SOLUTION\n",
    "    messages=[message]\n",
    "    # END_SOLUTION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anthropic_response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67fd35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
